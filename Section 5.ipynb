{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5: Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "**Teachers NOTES:** \n",
    "\n",
    "- Noun Phrase and Verb Phrase definitons\n",
    "- Need to keep examples simple enough for students to understand\n",
    "- Prepositions, adjectives, conjunctions need to be defined for the natural language understanding part of the course\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### OSEMN\n",
    "\n",
    "Data science can be broken down into the following 5 steps:\n",
    " 1. Obtaining\n",
    " 2. Scrubbing\n",
    " 3. Exploring\n",
    " 4. Modelling\n",
    " 5. Interpreting the data\n",
    "\n",
    "----\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtaining data\n",
    " - In general - getting the data from a few different sources\n",
    "    - e.g. Web pages, SQL Databases, APIs such as Twitter or Facebook\n",
    " - OR generating your own data (such as sensor data, or dimensional data you control)\n",
    "    - Special note: some simple python libraries to obtain and scrub data from various data sources\n",
    "    - NOTE: Pandas’ data frames should be used here - though need to figure out how to reconcile with the modelling and exploration sections later on\n",
    "    - Special note: Preparing data for analysis - when and how to use a large distributed framework, and when a single python-like installation is sufficient\n",
    "    - Special note: discussion of what a frame is - BlinkDB, MLLib, Spark, Infobright, Teradata, Postgres, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrubbing data\n",
    "\n",
    " - Filtering the data, extracting meaningful fields, replacing values, validating data\n",
    " - Data validation can be a tremendous task by itself\n",
    " - Different formats can be difficult to manage\n",
    " - We won’t spend much time here, but this is one of the most significant things that you’ll have to account for in the real world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring data\n",
    "\n",
    " - Sample - creating slices, samples, meaningful selections\n",
    " - Summarize - detailed summaries and statistics of what you see\n",
    " - See - visualize and test your hypothesis\n",
    " - **Special note:** p-values and hypothesis testing. Applying cross validation\n",
    " - **Special note:** An alternative to p-values and null hypotheses - Confidence Intervals\n",
    "\n",
    "**Example: using the command line**\n",
    "**Example: using matplotlib and iPython**\n",
    "**Special notes - Understanding dimensions and feature selection:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimensions and features. \n",
    "  - What is a dimension? What is a feature? What are predictors? What are variables?\n",
    "  - The curse of dimensionality - when and how to avoid too many dimensions. \n",
    "  - Dimension selection and elimination. \n",
    "  - Using scikit-learn to eliminate dimensionality using t-SNE (t-distributed stochastic neighbour embedding)\n",
    "  - Overfitting - eliminating noise and randomness. \n",
    "       - An example of overfitting census data, and how to remove it using Bayes’ Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using Pandas to capture and model our data\n",
    "  - An introduction to data frames\n",
    "  - Data Indexing in Pandas\n",
    "  - Data selection and aggregation - lambda functions and coding for scale. \n",
    "  - Migration of a data frame to a large scale production environment\n",
    "     - apply, applymap, groupby\n",
    "  - Summarizing data frames\n",
    "     - An introduction to data omission, data containment\n",
    "     - Automatically summarizing data frames (count, min, mean, etc.)\n",
    "     - Creating a simple persistent (and really fast) database using Pandas and Python 3\n",
    "     - Using Pandas and matplotlib to see your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelling the data\n",
    "  - Essentially a pattern matching process\n",
    "  - Detect patterns\n",
    "  - Explain the reasons for the patterns that you see\n",
    "  - Predict new events, patterns\n",
    "  - Generally a statistical model\n",
    "  - This is where we apply classification models, etc.\n",
    "\n",
    "----\n",
    "  Special Note: Principal Component Analysis - reducing dimensionality and clustering our data\n",
    "  - Special Note: Cluster Analysis - using k-means clustering to remove dimensionality and gather data into meaningful segments\n",
    "\n",
    "----\n",
    "  **Special Note: Bayes Classification**\n",
    "     - Classification\n",
    "     - Clustering\n",
    "     - Regression\n",
    "         - It is here that we encounter the basic problems that we are trying to address (see earlier notes in exploration):\n",
    "     - Dimensionality\n",
    "     - Overfitting\n",
    "     - Overtraining\n",
    "  **Special Note: Clustering**\n",
    "            - We will focus on one area of modelling: Clustering\n",
    "            - When we cluster data, we take a collection of items and partition them into smaller collections\n",
    "            - The smaller collections are called clusters\n",
    "            - The criteria by which we cluster the data are usually by some heuristic or rule\n",
    "            - Clustering is based primarily on comparison\n",
    "            - When we are clustering data, there are a number of problems that we may encounter\n",
    "            - Remember our data scrubbing?\n",
    "            - Well, clustering is based on comparisons\n",
    "            - This means that we rely on the data being the same in meaningful ways: for example, when we compare terms, that there are not equivalent terms that mean different things\n",
    "            - For example “Mr.” vs. “Mister”\n",
    "            - When we’re looking at words, we can address the problem by:\n",
    "                - Normalizing the data (e.g. changing “Mr.” to “Mister”)\n",
    "                - Calculating similarity - either at the syntactic level, or the semantic level. This is often called a similarity metric\n",
    "                    - For example, if we understand the concept of a “house”, a “home”, a “demesne” we can align our algorithm to accommodate for these different changes\n",
    "    - Interpreting the results\n",
    "\n",
    "        - Figure out what your results mean\n",
    "        - Plan actions if the process is manual\n",
    "        - Create an automatic feedback loop if you are sure of your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
